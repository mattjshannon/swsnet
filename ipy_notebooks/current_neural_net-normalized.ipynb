{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF neural net with normalized ISO spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from IPython.core.debugger import set_trace as st\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time\n",
    "\n",
    "# My modules\n",
    "from swsnet import helpers\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: ISO-SWS (normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed directories\n",
    "base_dir = '../data/isosws_atlas/'\n",
    "\n",
    "# Pickles containing our spectra in the form of pandas dataframes:\n",
    "spec_dir = base_dir + 'spectra_normalized/'\n",
    "spec_files = np.sort(glob.glob(spec_dir + '*.pkl'))\n",
    "\n",
    "# Metadata pickle (pd.dataframe). Note each entry contains a pointer to the corresponding spectrum pickle.\n",
    "metadata = base_dir + 'metadata_normalized.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels ('group'):\n",
    "\n",
    "1. Naked stars\n",
    "2. Stars with dust\n",
    "3. Warm, dusty objects\n",
    "4. Cool, dusty objects\n",
    "5. Very red objects\n",
    "6. Continuum-free objects but having emission lines\n",
    "7. Flux-free and/or fatally flawed spectra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B., these are shifted down by 1 in the labels (to span 0-6) for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset 1: all data included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = helpers.load_data(base_dir=base_dir, metadata=metadata, clean=False, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1235, 359)\n",
      "(1235,)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset 2: exclude group=7 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_clean, labels_clean = helpers.load_data(base_dir=base_dir, metadata=metadata, clean=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1058, 359)\n",
      "(1058,)\n"
     ]
    }
   ],
   "source": [
    "print(features_clean.shape)\n",
    "print(labels_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural(features, labels, l2norm=0.01):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(features, labels, test_size=0.3, random_state = 42)\n",
    "\n",
    "    # Sequential model, 7 classes of output.\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(l2norm), input_dim=359))\n",
    "    model.add(keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(l2norm)))\n",
    "    model.add(keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(l2norm)))\n",
    "    model.add(keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(l2norm)))\n",
    "    model.add(keras.layers.Dense(7, activation='softmax'))\n",
    "\n",
    "    # Early stopping condition.\n",
    "    callback = [tf.keras.callbacks.EarlyStopping(monitor='acc', patience=3, verbose=0)]\n",
    "\n",
    "    # Recompile model and fit.\n",
    "    model.compile(optimizer=keras.optimizers.Adam(0.0005),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    #     model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=False)\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32, callbacks=callback, verbose=False)\n",
    "\n",
    "    # Check accuracy.\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    accuracy = score[1]\n",
    "    print(\"L2 norm, accuracy: \", l2norm, accuracy)\n",
    "    \n",
    "    return model, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 norm, accuracy:  0.1 0.3854447448992665\n",
      "L2 norm, accuracy:  0.01 0.7277628046804361\n",
      "L2 norm, accuracy:  0.001 0.7708894893165548\n",
      "L2 norm, accuracy:  0.0001 0.7412398936292232\n",
      "L2 norm, accuracy:  1e-05 0.7304582224701935\n"
     ]
    }
   ],
   "source": [
    "for l2norm in (0.1, 0.01, 0.001, 0.0001, 0.00001):\n",
    "    model, accuracy = neural(features, labels, l2norm=l2norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 norm, accuracy:  0.1 0.4874213840226707\n",
      "L2 norm, accuracy:  0.01 0.7547169826315634\n",
      "L2 norm, accuracy:  0.001 0.8176100640176976\n",
      "L2 norm, accuracy:  0.0001 0.7798742123369901\n",
      "L2 norm, accuracy:  1e-05 0.789308174601141\n"
     ]
    }
   ],
   "source": [
    "for l2norm in (0.1, 0.01, 0.001, 0.0001, 0.00001):\n",
    "    model, accuracy = neural(features_clean, labels_clean, l2norm=l2norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural2(features, labels, l2norm=0.01):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(features, labels, test_size=0.25, random_state = 22)\n",
    "\n",
    "    # Sequential model, 7 classes of output.\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(l2norm), input_dim=359))\n",
    "    model.add(keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(l2norm)))    \n",
    "    model.add(keras.layers.Dense(6, activation='softmax'))\n",
    "\n",
    "    # Early stopping condition.\n",
    "    callback = [tf.keras.callbacks.EarlyStopping(monitor='acc', patience=3, verbose=0)]\n",
    "\n",
    "    # Recompile model and fit.\n",
    "    model.compile(optimizer=keras.optimizers.Adam(0.0005),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    #     model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=False)\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32, callbacks=callback, verbose=False)\n",
    "\n",
    "    # Check accuracy.\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    accuracy = score[1]\n",
    "    print(\"L2 norm, accuracy: \", l2norm, accuracy)\n",
    "    \n",
    "    return model, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 norm, accuracy:  0.1 0.5789473660079091\n",
      "L2 norm, accuracy:  0.01 0.6963562728905002\n",
      "L2 norm, accuracy:  0.001 0.7530364394187927\n",
      "L2 norm, accuracy:  0.0001 0.7246963548274176\n",
      "L2 norm, accuracy:  1e-05 0.7449392698071746\n"
     ]
    }
   ],
   "source": [
    "for l2norm in (0.1, 0.01, 0.001, 0.0001, 0.00001):\n",
    "    model, accuracy = neural2(features, labels, l2norm=l2norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 norm, accuracy:  0.1 0.6415094348619569\n",
      "L2 norm, accuracy:  0.01 0.724528302561562\n",
      "L2 norm, accuracy:  0.001 0.784905661052128\n",
      "L2 norm, accuracy:  0.0001 0.8113207556166739\n",
      "L2 norm, accuracy:  1e-05 0.8113207556166739\n"
     ]
    }
   ],
   "source": [
    "# using... 128, 64, 6. test_size=0.20, epoch=50, patience=5.\n",
    "for l2norm in (0.1, 0.01, 0.001, 0.0001, 0.00001):\n",
    "    model, accuracy = neural2(features_clean, labels_clean, l2norm=l2norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 norm, accuracy:  0.1 0.6289308194844228\n",
      "L2 norm, accuracy:  0.01 0.779874214586222\n",
      "L2 norm, accuracy:  0.001 0.8113207498436454\n",
      "L2 norm, accuracy:  0.0001 0.8113207494687734\n",
      "L2 norm, accuracy:  1e-05 0.8301886743719473\n"
     ]
    }
   ],
   "source": [
    "# using... 128, 64, 6. test_size=0.15, epoch=50, patience=5.\n",
    "for l2norm in (0.1, 0.01, 0.001, 0.0001, 0.00001):\n",
    "    model, accuracy = neural2(features_clean, labels_clean, l2norm=l2norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above, probably need to do more data preprocessing:\n",
    "- e.g., remove untrustworthy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '../models/neural_net.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
